<!-- ðŸš€ Features -->

âœ… Generate text embeddings using Xenova/all-MiniLM-L6-v2

âœ… Store embeddings in Qdrant vector database

âœ… Perform semantic search (Cosine similarity)

âœ… Reject irrelevant queries using similarity thresholds

âœ… Confidence gap logic to avoid ambiguous results

âœ… REST API built with Express

<!-- ðŸ§  Architecture Overview -->
Client
  â”‚
  â”œâ”€ POST /add       â†’ Generate embedding â†’ Store in Qdrant
  â”‚
  â””â”€ POST /chat    â†’ Generate embedding â†’ Vector search â†’ Best match

  <!-- ðŸ› ï¸ Tech Stack -->

Node.js (ES Modules)

Express

@xenova/transformers â€“ Local embedding generation

Qdrant â€“ Vector database

Cosine Similarity

Nodemon â€“ Dev workflow

<!-- âœ… Checklist before running curl -->

âœ” Qdrant running â†’ ./qdrant
âœ” Backend running â†’ node server.js
âœ” Using 127.0.0.1 in vectorStore
âœ” await searchVector() present

<!-- ðŸŽ¯ Similarity Logic -->

Similarity Threshold: 0.45
Confidence Gap: 0.05

<!-- A result is rejected if: -->
Best score < threshold
Difference between best and second-best score is too small
Payload text is missing

<!-- This helps avoid: -->
Hallucinations
Weak semantic matches
Ambiguous results

<!-- ðŸ§  Embedding Model -->

Model: Xenova/all-MiniLM-L6-v2
Vector Size: 384
Pooling: Mean
Normalization: Enabled
Embeddings are generated locally, no external API calls.

User Question
   â†“
Embedding
   â†“
Vector Search (Context)
   â†“
Prompt Construction
   â†“
LLM Answer (grounded in context)

<!-- ðŸ§  Chat with Documents (RAG) -->
Endpoint
POST /api/documents/chat

âœ… Example: Valid Question
curl -X POST http://localhost:3000/api/documents/chat \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is a Large Language Model?"
  }' | jq

<!-- Response -->
{
  "answer": "A Large Language Model is a powerful AI system that understands and generates human-like text.",
  "source": "Large Language Models are powerful AI systems.",
  "similarity": 0.72
}

<!-- ðŸš« Example: Irrelevant Question -->
curl -X POST http://localhost:3000/api/documents/chat \
  -H "Content-Type: application/json" \
  -d '{
    "query": "How to cook pasta?"
  }' | jq

<!-- Response -->
{
  "answer": "I don't know",
  "source": null
}  

<!-- ðŸ§ª Complete Flow Using curl -->
<!-- 1ï¸âƒ£ Add Document -->
curl -X POST http://localhost:3000/api/documents/add \
  -H "Content-Type: application/json" \
  -d '{
    "id": 1,
    "text": "Large Language Models are powerful AI systems used for natural language understanding."
  }' | jq

<!-- 2ï¸âƒ£ Chat with RAG -->
curl -X POST http://localhost:3000/api/documents/chat \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are LLMs used for?"
  }' | jq
